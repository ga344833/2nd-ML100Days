{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist  \n",
    "from keras.utils import np_utils  \n",
    "import numpy as np  \n",
    "np.random.seed(10)  \n",
    "  \n",
    "# Read MNIST data  \n",
    "(X_Train, y_Train), (X_Test, y_Test) = mnist.load_data()\n",
    "\n",
    "y_Test.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#載入大小為60000張 28 * 28大小的圖片 且像素值介於0~255 標準化後在0~1\n",
    "  \n",
    "# Translation of data  \n",
    "X_Train40 = X_Train.reshape(X_Train.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "\n",
    "#因為CNN有捲積與池化運算,必須保留影像的維度.因此將資料轉成28 * 28 * 1(高)的影像單位\n",
    "\n",
    "\n",
    "X_Test40 = X_Test.reshape(X_Test.shape[0], 28, 28, 1).astype('float32')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(60000, 10)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Standardize feature data\n",
    "\n",
    "#標準化即是將原本的數據作線性轉化, 目的是要將圖片灰度與曝光度給忽略 ,讓電腦不會因此\n",
    "# 辨識錯誤\n",
    "# https://zhuanlan.zhihu.com/p/35597976\n",
    "#標準化可以使圖片映射至同一個特定區間,讓CNN可以對數據進行同樣的處理\n",
    "#就像....同一起跑點的感覺\n",
    "\n",
    "\n",
    "X_Train40_norm = X_Train40 / 255  \n",
    "X_Test40_norm = X_Test40 / 255  \n",
    "\n",
    "# 為何除255? \n",
    "# 1個點像素值為0~255 , 標準化後數值降低 , ???\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "# Label Onehot-encoding\n",
    "\n",
    "# one hot 編碼 : 機器學習中,特徵有時並不是連續值,有可能是分類值(像是男,女而不是123)\n",
    "\n",
    "# 對這類特徵 我們需要將其數字化才能放入學習\n",
    "\n",
    "# 就使用到 one hot encoding \n",
    "\n",
    "# 已知運作 還不知為何用了就能丟進去學習\n",
    "\n",
    "# https://blog.csdn.net/taotiezhengfeng/article/details/73692239\n",
    "\n",
    "#簡單來說就是 將各個特徵值標示為1而不是2 3 4等等\n",
    "# 是讓最初訓練時每個特徵間的距離都相同而做的\n",
    "\n",
    "# \n",
    "\n",
    "# https://www.itread01.com/content/1501748526.html\n",
    "y_TrainOneHot = np_utils.to_categorical(y_Train)  \n",
    "print(y_Train[0])\n",
    "print(y_TrainOneHot.shape)\n",
    "y_TestOneHot = np_utils.to_categorical(y_Test)\n",
    "print(y_Test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D  \n",
    "from keras.models import Model\n",
    "#  從Keras呼叫Conv2D ,與 Maxpooling2D\n",
    "# 先卷積:\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# Create CN layer 1  \n",
    "model.add(Conv2D(filters=16,#filter 產生16個隨機的小方塊\n",
    "                 kernel_size=(5,5), #卷積方塊大小為5 * 5 = 25\n",
    "                 \n",
    "                 \n",
    "                 padding='same',  #將卷積後縮小的feature map 補上0 就像畫框一樣\n",
    "                 \n",
    "# 又被稱作zero-padding\n",
    "#                  為何要補??\n",
    "                 \n",
    "#                  因為他能夠抑制圖像邊緣的效果，\n",
    "#                  一般情況下我們認為圖像的中間部分比起周圍更重要。\n",
    "                 \n",
    "                 \n",
    "                 input_shape=(28,28,1),  \n",
    "                 activation='relu'))  #使用激活函數主要是引入表達能力(非線性)\n",
    "                                        # 之後研究\n",
    "    \n",
    "# 再池化:\n",
    "\n",
    "\n",
    "# Create Max-Pool 1  \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#pooling是保存重要特徵並減少feature map 的大小\n",
    "\n",
    "# 藉由減少大小可加快運作效率 也能避免過擬合的情況發生\n",
    "\n",
    "  \n",
    "# Create CN layer 2  \n",
    "model.add(Conv2D(filters=36,  \n",
    "                 kernel_size=(5,5),  \n",
    "                 padding='same',  \n",
    "                 input_shape=(28,28,1),  \n",
    "                 activation='relu'))  \n",
    "  \n",
    "# Create Max-Pool 2  \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "  \n",
    "# Add Dropout layer  \n",
    "# dropput 函式可防止過擬合, 輸入參數則可丟棄一部份的檔案?\n",
    "# 首要參數rate為關掉隱藏層節點的比例，\n",
    "# 利用隨機關掉隱藏層節點與輸入神經元的連結，不更新權重(W)，\n",
    "# 造成多個結果，再作比較去除極端值，即可達到避免過度擬合的現象。\n",
    "\n",
    "model.add(Dropout(0.25))  \n",
    "\n",
    "\n",
    "# 建立神經網路\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# flatten : 將前面的東西拉平後 , 給NN做訓練\n",
    "\n",
    "# Hidden layer \n",
    "\n",
    "# 添加hidden layer 在中間\n",
    "\n",
    "# 主要功能為增加神經網路的複雜度,模擬非線性關系,不過太多會導致过擬合\n",
    "\n",
    "# dense即為常用的全連結層\n",
    "\n",
    "# 全連結層的作用即是分類\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# units 代表的是維度 \n",
    "\n",
    "\n",
    "# 128個特徵點 每個特徵點都向之前卷積池化後的feature map 取得不同的權重\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "\n",
    "# 建立輸出層\n",
    "model.add(Dense(10, activation='softmax')) \n",
    "# 使用softmax 可將輸出轉成每個數字的機率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 36)        14436     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 36)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 36)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1764)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               225920    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 242,062\n",
      "Trainable params: 242,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看模型\n",
    "model.summary()  \n",
    "print(\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "19200/48000 [===========>..................] - ETA: 18s - loss: 2.2137 - acc: 0.2401"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ebb6d0e45abf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m train_history = model.fit(x=X_Train40_norm,  \n\u001b[0;32m     14\u001b[0m                           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_TrainOneHot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                           epochs=10, batch_size=4800, verbose=1)  \n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# x : 數據\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型定義好了 接著要開始訓練\n",
    "\n",
    "# 1.定義訓練\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# 選擇損失函數 , 優化的方法 ,成效衡量方式\n",
    "\n",
    "# * loss: 設定 Loss Function, 這邊選定 Cross Entropy 作為 Loss Function.\n",
    "# * optimizer: 設定訓練時的優化方法, 在深度學習使用 adam (Adam: A Method for Stochastic Optimization) 可以更快收斂, 並提高準確率.\n",
    "# * metrics: 設定評估模型的方式是 accuracy 準確率.\n",
    "\n",
    "# 2.開始訓練\n",
    "\n",
    "train_history = model.fit(x=X_Train40_norm,  \n",
    "                          y=y_TrainOneHot, validation_split=0.2,  \n",
    "                          epochs=10, batch_size=480, verbose=1)  \n",
    "\n",
    "# x : 數據\n",
    "# y : 標籤\n",
    "\n",
    "# validation_split : 用來指定訓練集的一定比例數據做為驗證集 \n",
    "\n",
    "# epochs : 訓練在到達該值(10)的時候停止 ,沒設定就是訓練到底\n",
    "\n",
    "# batch_size : 每次訓練的資料筆數\n",
    "\n",
    "# verbose : 1 為 顯示epoch紀錄\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
